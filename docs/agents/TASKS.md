# TASKS (Single Source of Truth / Project Plan)

This document defines all current and future platform development work for **agent-runner**. It is the **single source of truth** for the project roadmap. Workflows like `quarkus-bootstrap-v1` are treated as demos/regression fixtures, not core deliverables for the platform itself.

## Rules for TASKS.md:
- **Only implement items explicitly listed under the "Now" section.** These are the highest priority tasks.
- If new ideas come up, put them in `INBOX.md` and propose them for triage to be moved here.
- The "Now" section must contain a **maximum of 1-2 active tasks** at any given time to maintain focus.
- **No platform code modifications are permitted without a corresponding task in `TASKS.md` under the "Now" section.**
- Each task must include a clear "Definition of Done" (DoD) and verifiable "Validation Commands".

---

## Now

### 1) Integrate with multiple LLM providers (Gemini, Claude, Ollama)
**Goal:** Connect the agent to multiple LLM providers to enable flexible, real LLM-powered execution.
**DoD:**
- Provider abstraction implemented for Gemini, Claude, and Ollama.
- Agent can successfully invoke each configured LLM provider.
- Basic API endpoint exists to list available LLM providers.
**Validation Commands:**
```bash
make test # Ensures backend integration and provider abstraction
# Use bruno tests or curl to hit the API endpoint to list providers
# Manually test agent execution with each configured LLM provider (e.g., using a simple "hello world" prompt)
```

### 2) Implement File System Operations
**Goal:** Allow agents to read from and write to the local file system in a controlled manner.
**DoD:**
- API endpoints exist for agent to read and write files within a designated project directory.
- Security considerations (e.g., path traversal) are addressed.
- Agent execution logic supports file system interactions.
**Validation Commands:**
```bash
make test # Ensures backend API for file operations
# Create a test workflow that involves the agent reading from and writing to a file, then verify file contents
# Use bruno tests or curl to test file read/write API endpoints
```

---

## Next

### 1) Define and enforce run/step state invariants
**Goal:** Make run + step state monotonic and consistent across API, worker, UI.
**DoD:**
- Run status transitions are monotonic: QUEUED → RUNNING → (COMPLETED | FAILED | CANCELED)
- Step state never regresses (no completed → running)
- Events are strictly append-only and ordered by id
- Tests cover the invariants
**Validation Commands:**
```bash
make test # Ensures API and worker logic
# Manually verify UI transitions for a run through its lifecycle (create, pause, resume, stop)
```
**Notes:**
- This is platform correctness; workflows are just inputs.

---

### 2) Standardize artifacts as first-class outputs (design first, implement minimal)
**Goal:** Every run can produce artifacts with metadata and retrieval.
We will *not* pick a final storage backend yet—start with a minimal abstraction.
**DoD:**
- Artifact metadata model exists (id, run_id, name, kind, path or uri, created_at)
- API can list artifacts for a run (`GET /runs/{id}/artifacts`)
- UI can display artifacts list (even if download is v2)
- Tests cover artifact creation + listing
- Artifact Management: Implement a system for managing artifacts generated by agent runs, including creation, listing, and basic metadata.
**Validation Commands:**
```bash
make test # Ensures API logic and model definitions
# Use bruno tests for /runs/{id}/artifacts endpoint
# Manually verify UI displays artifact list on run detail page
# Verify artifact creation via agent output and API listing
```
**Notes:**
- Keep storage abstract: local filesystem ok for now.

---

### 3) Add run cancellation (`POST /runs/{id}/cancel`) with cooperative worker checks
**Goal:** Enable graceful cancellation of running agent tasks.
**DoD:**
- API endpoint `POST /runs/{id}/cancel` is implemented and functional.
- Worker gracefully handles cancellation requests, stopping the agent's execution.
- Run status transitions correctly to `CANCELED`.
**Validation Commands:**
```bash
# Start agent and console
make start
# Create a run and then immediately call the cancel endpoint
# Verify run status changes to CANCELED and worker stops processing
# Check logs for graceful shutdown messages
```

---

### 4) Add LLM heartbeat events during long generations (great with SSE)
**Goal:** Provide real-time feedback during long-running LLM generation tasks.
**DoD:**
- LLM interactions emit periodic heartbeat events during long generations.
- Frontend displays these events via SSE for real-time updates.
**Validation Commands:**
```bash
# Start agent and console
make start
# Create a run with a long LLM generation task (e.g., a complex code generation)
# Monitor SSE stream for heartbeat events in the console's network tab or UI
```

---

### 5) Add workflow discovery endpoints (`GET /workflows`, `GET /workflows/{name}`)
**Goal:** Allow the console and other clients to discover available workflows.
**DoD:**
- API endpoints `GET /workflows` (list all available workflows) and `GET /workflows/{name}` (get details of a specific workflow) are implemented and functional.
- Workflows are correctly registered and discoverable.
**Validation Commands:**
```bash
# Use bruno tests or curl to hit /workflows and /workflows/{name} endpoints
# Verify correct listing and details of available workflows, including example/demo workflows
```

---

### 6) Add health endpoints (`/healthz`, `/readyz`) and basic deployment docs
**Goal:** Provide standard endpoints for service health checks and initial deployment guidance.
**DoD:**
- API endpoints `/healthz` (liveness) and `/readyz` (readiness) are implemented and return appropriate HTTP status codes (e.g., 200 OK for healthy).
- Basic deployment documentation is added to `docs/DEPLOYMENT.md` for common scenarios.
**Validation Commands:**
```bash
curl http://localhost:8000/healthz
curl http://localhost:8000/readyz
# Verify HTTP 200 OK for healthy service, other codes for unhealthy states
# Review docs/DEPLOYMENT.md for new content on basic deployment steps
```

---

### 7) Basic Git Operations
**Goal:** Implement basic Git operations for agents, such as creating branches and committing changes.
**DoD:**
- Agent can perform `git clone`, `git branch`, `git checkout`, `git add`, `git commit`, `git push` (to pre-configured remote) within a designated project directory.
- API endpoints exist to expose these capabilities to the agent.
- Security considerations (e.g., credential management, repository access) are addressed.
**Validation Commands:**
```bash
make test # Ensures backend API for git operations
# Create a test workflow where the agent clones a repo, makes changes, commits, and pushes, then verify changes in the remote repo.
# Use bruno tests or curl to test git operation API endpoints.
```

---

### 8) Multi-Agent Coordination
**Goal:** Implement a system for coordinating the execution of multiple agents.
**DoD:**
- A mechanism for agents to communicate and hand off tasks to each other is in place.
- Run state can reflect multi-agent coordination (e.g., parent/child runs).
- Basic API endpoints for agent-to-agent communication are available.
**Validation Commands:**
```bash
make test # Ensures backend coordination logic
# Create a simple multi-agent workflow where Agent A performs a task and hands off to Agent B, then verify successful execution of both.
# Monitor event logs for multi-agent coordination activities in the console.
```

---

### 9) Web Browsing Capabilities
**Goal:** Allow agents to browse the web to gather information.
**DoD:**
- Agent can make HTTP requests to external URLs (GET, POST).
- Content can be retrieved and parsed by the agent (e.g., HTML parsing, JSON extraction).
- Security (e.g., SSRF protection) and rate limiting considerations are addressed.
**Validation Commands:**
```bash
make test # Ensures web browsing utilities/API
# Create a test workflow where the agent fetches content from a known URL and extracts specific information, then verify the extracted data.
# Verify security mechanisms prevent unauthorized access or excessive requests.
```

---

### 10) Long-Term Memory
**Goal:** Implement a long-term memory solution for agents to retain information across runs.
**DoD:**
- A mechanism for agents to store and retrieve persistent information is implemented (e.g., vector database, key-value store).
- The memory solution can be accessed and updated by agents during runs.
- Data privacy and retention policies are considered and documented.
**Validation Commands:**
```bash
make test # Ensures memory solution integration
# Create a two-part workflow: Agent A stores a piece of information in memory, then Agent B (in a separate run) retrieves that information and acts upon it.
# Verify memory content via direct database inspection or dedicated API if available.
```

---

## Later

### 1) Persistence strategy decision (SQLite/Postgres) + migrations
**Goal:** Finalize the long-term persistence strategy and enable smooth transitions.
**DoD:**
- A clear decision is made on the primary persistence strategy (e.g., PostgreSQL is chosen).
- Database migration tools and scripts are in place to transition from SQLite to the chosen solution.
- Documentation exists for setting up and migrating the database.
**Validation Commands:**
```bash
# Review architectural documentation for persistence strategy decision and rationale
# Run database migration scripts in a test environment and verify data integrity and application functionality post-migration
```

---

### 2) Artifact download endpoint + durable artifact storage options
**Goal:** Enable users to download generated artifacts and store them durably.
**DoD:**
- API endpoint for downloading specific artifacts (`GET /runs/{id}/artifacts/{artifact_id}/download`) is implemented and functional.
- Configuration for durable artifact storage (e.g., S3, Google Cloud Storage) is available and tested.
- Integration allows storing and retrieving artifacts from the chosen durable storage.
**Validation Commands:**
```bash
# Use bruno tests or curl to download an artifact via the new endpoint
# Configure and test integration with a durable storage option (e.g., upload an artifact, then download and verify integrity)
```

---

### 3) Auth (optional, basic)
**Goal:** Provide basic authentication/authorization for API access.
**DoD:**
- A basic authentication/authorization mechanism (e.g., API keys, simple token-based auth) is implemented for API access.
- Protected endpoints require valid credentials.
**Validation Commands:**
```bash
# Attempt to access protected API endpoints without authentication; verify denial (HTTP 401/403)
# Attempt to access protected API endpoints with valid credentials; verify success (HTTP 200)
```

---

### 4) Provider abstraction growth (future providers)
**Goal:** Ensure the LLM provider abstraction is flexible and extensible for future integrations.
**DoD:**
- The LLM provider abstraction is designed to easily integrate new LLMs beyond the initial set (Gemini, Claude, Ollama).
- Clear documentation exists for developers to add new LLM providers to the platform.
**Validation Commands:**
```bash
# Review provider abstraction code for extensibility (e.g., interface, factory pattern)
# Review documentation for clarity and completeness on how to add new LLM providers
# (Optional) Implement a dummy new provider using the abstraction and verify its integration points
```

---

### 5) Forgejo Integration
**Goal:** Automate Git-related tasks through integration with Forgejo.
**DoD:**
- Integration with Forgejo for automated Git-related tasks (e.g., repository creation, webhooks for event triggers, commit status updates) is implemented.
- Agent Runner can interact with Forgejo APIs.
**Validation Commands:**
```bash
# Configure Forgejo and Agent Runner with necessary credentials
# Create a new project/repository in Forgejo via Agent Runner API/UI and verify its creation
# Set up a webhook from Forgejo to Agent Runner and verify Agent Runner receives and processes it (e.g., updating run status based on commit)
```

---

### 6) Taiga Integration
**Goal:** Integrate with Taiga for project management and issue tracking.
**DoD:**
- Integration with Taiga for project management and issue tracking (e.g., task creation, status updates, comment posting) is implemented.
- Agent Runner can interact with Taiga APIs.
**Validation Commands:**
```bash
# Configure Taiga and Agent Runner with necessary credentials
# Create a task in Taiga via Agent Runner API/UI and verify its creation
# Verify Agent Runner can update Taiga task status or add comments based on agent activity
```

---

### 7) Custom Workflows
**Goal:** Allow users to define and upload custom agent workflows.
**DoD:**
- A mechanism for users to define and upload custom workflows (e.g., YAML, DSL) is implemented.
- The Agent Runner can parse, validate, and execute these custom workflows.
**Validation Commands:**
```bash
# Define a simple custom workflow in the specified format
# Upload the custom workflow via API/UI
# Execute the custom workflow and verify correct behavior and outputs
```

---

### 8) Authentication and Authorization (Robust)
**Goal:** Implement a robust, comprehensive authentication and authorization system.
**DoD:**
- A robust authentication and authorization system (e.g., JWT-based, OAuth2 with user management) is implemented.
- All API endpoints and UI access are protected by fine-grained access controls.
- User and role management functionalities are available.
**Validation Commands:**
```bash
# Conduct security tests for various authentication and authorization bypasses (e.g., penetration testing)
# Verify role-based access control functions correctly across different user roles
# Test user registration, login, and token management flows
```

---

### 9) Scalability Improvements
**Goal:** Optimize the platform for performance and scalability under load.
**DoD:**
- Key components (e.g., worker pool, database connections, event processing) are optimized for performance and scalability.
- Load testing demonstrates improved performance (e.g., higher throughput, lower latency) under defined high concurrency scenarios.
- Bottlenecks are identified and addressed.
**Validation Commands:**
```bash
# Define load testing scenarios and metrics (e.g., using k6, JMeter)
# Run load tests with a defined number of concurrent runs/users and measure performance
# Analyze performance metrics (e.g., CPU, memory, database query times) and verify improvements
```

---

### 10) Monitoring and Alerting
**Goal:** Provide comprehensive visibility into system health and performance.
**DoD:**
- A comprehensive monitoring solution (e.g., Prometheus for metrics, Grafana for dashboards, Alertmanager for alerts) is integrated.
- Key system metrics (e.g., run status, worker load, API latency, resource usage) are collected and visualized in dashboards.
- Critical alerts are configured to notify on predefined thresholds (e.g., service downtime, high error rates).
**Validation Commands:**
```bash
# Deploy monitoring solution components
# Verify dashboards display relevant metrics in real-time
# Trigger alerts (e.g., by simulating a service failure or exceeding a threshold) and verify notifications are received
```

---

### 11) Deployment Automation
**Goal:** Automate the deployment process for Agent Runner across various environments.
**DoD:**
- Automated scripts or CI/CD pipelines are in place for deploying Agent Runner to various environments (e.g., Kubernetes, Docker Swarm, cloud platforms).
- The deployment process is reproducible and minimizes manual intervention.
- Deployment documentation for different environments is available.
**Validation Commands:**
```bash
# Execute the deployment pipeline to a staging environment (e.g., a Kubernetes cluster)
# Verify successful deployment, proper configuration, and functionality of the application in the target environment
# Review deployment documentation for clarity and accuracy
```